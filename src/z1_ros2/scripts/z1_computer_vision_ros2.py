#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CompressedImage
from geometry_msgs.msg import Twist, Point
from std_msgs.msg import String
from cv_bridge import CvBridge
import cv2
import numpy as np
from ultralytics import YOLO
import threading

class Z1ComputerVisionROS2(Node):
    def __init__(self):
        super().__init__('z1_computer_vision_ros2')
        
        # Publishers
        self.cmd_pub = self.create_publisher(Twist, '/z1/cmd_vel', 10)
        self.detection_pub = self.create_publisher(String, '/z1/detections', 10)
        self.annotated_image_pub = self.create_publisher(CompressedImage, '/z1/annotated_image/compressed', 10)
        
        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10
        )
        
        # CV Bridge
        self.bridge = CvBridge()
        
        # YOLO model
        try:
            self.yolo_model = YOLO('yolov8n.pt')  # Lightweight model
            self.use_yolo = True
            self.get_logger().info('YOLO model loaded successfully')
        except Exception as e:
            self.get_logger().warn(f'YOLO not available: {e}. Using color detection fallback.')
            self.use_yolo = False
        
        # Vision parameters\n        self.target_object = 'bottle'  # Default target\n        self.tracking_enabled = False\n        self.last_detection = None\n        \n        # Color detection parameters (fallback)\n        self.color_ranges = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'green': ([40, 50, 50], [80, 255, 255]),\n            'blue': ([100, 50, 50], [130, 255, 255])\n        }\n        \n        self.get_logger().info('Z1 Computer Vision ROS2 initialized')\n    \n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n            \n            # Perform detection\n            if self.use_yolo:\n                detections, annotated_image = self.yolo_detection(cv_image)\n            else:\n                detections, annotated_image = self.color_detection(cv_image)\n            \n            # Publish detections\n            if detections:\n                detection_msg = String()\n                detection_msg.data = str(detections)\n                self.detection_pub.publish(detection_msg)\n                \n                # Track target if enabled\n                if self.tracking_enabled:\n                    self.track_target(detections, cv_image.shape)\n            \n            # Publish annotated image\n            self.publish_annotated_image(annotated_image)\n            \n        except Exception as e:\n            self.get_logger().error(f'Image processing error: {e}')\n    \n    def yolo_detection(self, image):\n        \"\"\"YOLO object detection\"\"\"\n        results = self.yolo_model(image)\n        detections = []\n        annotated_image = image.copy()\n        \n        for result in results:\n            boxes = result.boxes\n            if boxes is not None:\n                for box in boxes:\n                    # Get detection info\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    confidence = box.conf[0].cpu().numpy()\n                    class_id = int(box.cls[0].cpu().numpy())\n                    class_name = self.yolo_model.names[class_id]\n                    \n                    if confidence > 0.5:  # Confidence threshold\n                        # Calculate center\n                        center_x = int((x1 + x2) / 2)\n                        center_y = int((y1 + y2) / 2)\n                        \n                        detections.append({\n                            'class': class_name,\n                            'confidence': float(confidence),\n                            'center': (center_x, center_y),\n                            'bbox': (int(x1), int(y1), int(x2), int(y2))\n                        })\n                        \n                        # Draw bounding box\n                        cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n                        cv2.putText(annotated_image, f'{class_name}: {confidence:.2f}', \n                                  (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n                        cv2.circle(annotated_image, (center_x, center_y), 5, (0, 0, 255), -1)\n        \n        return detections, annotated_image\n    \n    def color_detection(self, image):\n        \"\"\"Color-based object detection (fallback)\"\"\"\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        detections = []\n        annotated_image = image.copy()\n        \n        for color_name, (lower, upper) in self.color_ranges.items():\n            # Create mask\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n            \n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 500:  # Minimum area threshold\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n                    \n                    detections.append({\n                        'class': f'{color_name}_object',\n                        'confidence': 0.8,  # Fixed confidence for color detection\n                        'center': (center_x, center_y),\n                        'bbox': (x, y, x + w, y + h)\n                    })\n                    \n                    # Draw detection\n                    cv2.rectangle(annotated_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n                    cv2.putText(annotated_image, f'{color_name}_object', \n                              (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n                    cv2.circle(annotated_image, (center_x, center_y), 5, (0, 0, 255), -1)\n        \n        return detections, annotated_image\n    \n    def track_target(self, detections, image_shape):\n        \"\"\"Track and follow detected target\"\"\"\n        height, width = image_shape[:2]\n        image_center_x = width // 2\n        image_center_y = height // 2\n        \n        # Find target object\n        target_detection = None\n        for detection in detections:\n            if self.target_object in detection['class'].lower():\n                target_detection = detection\n                break\n        \n        if target_detection:\n            center_x, center_y = target_detection['center']\n            \n            # Calculate error from image center\n            error_x = center_x - image_center_x\n            error_y = center_y - image_center_y\n            \n            # Create control command\n            twist = Twist()\n            \n            # Proportional control\n            kp_angular = 0.001  # Proportional gain for rotation\n            kp_linear = 0.0005  # Proportional gain for forward/backward\n            \n            # Horizontal tracking (rotate base)\n            twist.angular.z = -kp_angular * error_x\n            \n            # Vertical tracking (move shoulder)\n            twist.linear.y = -kp_linear * error_y\n            \n            # Distance-based forward movement\n            bbox_area = (target_detection['bbox'][2] - target_detection['bbox'][0]) * \\\n                       (target_detection['bbox'][3] - target_detection['bbox'][1])\n            \n            # Move closer if object is small (far away)\n            if bbox_area < 5000:\n                twist.linear.x = 0.1\n            elif bbox_area > 20000:\n                twist.linear.x = -0.1\n            \n            # Publish control command\n            self.cmd_pub.publish(twist)\n            \n            self.get_logger().info(f'Tracking {target_detection[\"class\"]} at ({center_x}, {center_y})')\n        else:\n            # Stop if target not found\n            self.cmd_pub.publish(Twist())\n    \n    def publish_annotated_image(self, image):\n        \"\"\"Publish annotated image\"\"\"\n        try:\n            # Convert to compressed image\n            _, buffer = cv2.imencode('.jpg', image)\n            \n            compressed_msg = CompressedImage()\n            compressed_msg.header.stamp = self.get_clock().now().to_msg()\n            compressed_msg.format = 'jpeg'\n            compressed_msg.data = buffer.tobytes()\n            \n            self.annotated_image_pub.publish(compressed_msg)\n        except Exception as e:\n            self.get_logger().error(f'Image publishing error: {e}')\n    \n    def set_target(self, target_name):\n        \"\"\"Set target object for tracking\"\"\"\n        self.target_object = target_name\n        self.get_logger().info(f'Target set to: {target_name}')\n    \n    def enable_tracking(self, enable=True):\n        \"\"\"Enable/disable object tracking\"\"\"\n        self.tracking_enabled = enable\n        status = 'enabled' if enable else 'disabled'\n        self.get_logger().info(f'Object tracking {status}')\n        \n        if not enable:\n            # Stop robot when tracking disabled\n            self.cmd_pub.publish(Twist())\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    try:\n        cv_node = Z1ComputerVisionROS2()\n        \n        # Example: Enable tracking after 5 seconds\n        def enable_tracking_delayed():\n            import time\n            time.sleep(5)\n            cv_node.enable_tracking(True)\n            cv_node.set_target('bottle')  # Track bottles\n        \n        tracking_thread = threading.Thread(target=enable_tracking_delayed)\n        tracking_thread.daemon = True\n        tracking_thread.start()\n        \n        rclpy.spin(cv_node)\n        \n    except KeyboardInterrupt:\n        pass\n    finally:\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()